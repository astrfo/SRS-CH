{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PaperSRS.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KXHprgMOajBO",
        "Zz_lD-JLobFm",
        "wr8pWRq4wA7s",
        "73qbNC6IzDUh"
      ],
      "authorship_tag": "ABX9TyPOsoWLaCL6n18zL7eUWkQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrfo/SRS-CH/blob/main/PaperSRS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMRT_k5vf2Ak"
      },
      "source": [
        "# 満足化方策における非満足均衡を用いた確率的方策の検証\n",
        "\n",
        "[Link] https://www.jstage.jst.go.jp/article/pjsai/JSAI2021/0/JSAI2021_1G2GS2a04/_article/-char/ja/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaPmWlGNgFbd"
      },
      "source": [
        "## 1. はじめに"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiMPMX9ZgJ6A"
      },
      "source": [
        "## 2. 多腕バンディット問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXHprgMOajBO"
      },
      "source": [
        "## 3. 満足化方策 RS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3GDr04c0HX"
      },
      "source": [
        "### 3.1 RS の定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69DmDL0Rc68R"
      },
      "source": [
        "### 3.2 非定常環境における RS\n",
        "\n",
        "RS は特定の選択肢の試行回数 $n_i$ が他の選択肢の試行回数に比べ充分に大きいと、ハズレを引きまくっても優先順位が切り替わりにくい。それによって非定常環境では選ばれるべき選択肢への切り替えが遅い。そこで「忘却率 $\\gamma$」を導入し、以下のように更新する。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "w_i &=& \\left\\{\n",
        "\\begin{array}{lcl}\n",
        "    \\gamma w_i + 1 \\quad (a_i = a_s \\; \\& \\; r = 1) \\\\\n",
        "    \\gamma w_i      \\quad (otherwise) \\\\\n",
        "\\end{array}\n",
        "\\right. \\qquad \\cdots　(7) \\\\\n",
        "l_i &=& \\left\\{\n",
        "\\begin{array}{lcl}\n",
        "    \\gamma l_i + 1 \\quad (a_i = a_s \\; \\& \\; r = 0) \\\\\n",
        "    \\gamma l_i      \\quad (otherwise) \\\\\n",
        "\\end{array}\n",
        "\\right. \\qquad \\cdots (8) \\\\\n",
        "n_i &=& w_i + l_i \\qquad \\cdots　(9) \\\\\n",
        "E_i &=& \\frac{w_i}{w_i + l_i} \\qquad \\cdots　(10) \\\\\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "式(7), (8)のように忘却率 $\\gamma$ をかけて徐々に過去の情報を忘却させる。これを RS$\\gamma$ と呼ぶ。この論文では RS と SRS 双方に忘却率 $\\gamma = 0.999$ を導入する。実は定常環境でも RS$\\gamma$ は優れた性能を見せるらしい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_lD-JLobFm"
      },
      "source": [
        "## 4. 確率的満足化方策 SRS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M_nx8O7oevf"
      },
      "source": [
        "### 4.1 非満足状況\n",
        "\n",
        "**いずれも$E_i < \\aleph$を満たしているとき、これを非満足状況と定義する。**非満足状況時、特定の選択肢の試行量割合が高ければ高いほど(その腕が他の腕に比べてめっちゃ選ばれてる)、その選択肢から算出される RS 評価値は低下していく。対して、その他の選択肢の RS 評価値は上昇していく。この上昇率は試行量割合が増加していけばいくほど緩やかになっていく。そのため総試行量 $N$ が十分大きくなると 全ての RS 評価値が均衡値 $-Z$ になる。これによって試行量割合 $\\rho_i$ は以下のように逆算することができる。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "RS_i &=& -Z \\qquad \\cdots (11) \\\\\n",
        "\\rho_i &=& \\frac{n_i}{N} = \\frac{Z}{\\aleph - E_i} \\qquad \\cdots (12) \\\\\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "また試行量割合 $\\rho_i$ の総和が 1 になることから $Z$ を以下のように算出できる。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\sum_{i=1}^{K}\\rho_i &=& \\sum_{i=1}^{K}\\frac{Z}{\\aleph - E_i} = 1 \\qquad \\cdots (13) \\\\\n",
        "Z &=& \\frac{1}{\\sum_{i=1}^{K}\\frac{Z}{\\aleph - E_i}} \\qquad \\cdots (14) \\\\\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "試行量割合 $\\rho_i$ は基準値 $\\aleph$ と経験期待値 $E_i$ から定義できる。この RS 均衡値 $-Z$ を用いて求められる試行量割合を $\\rho^z_i$ とし、**SRS 値の算出にはこれを用いる。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmtPXhRmuMH8"
      },
      "source": [
        "### 4.2 SRS 値の算出方法\n",
        "\n",
        "SRS は現在の試行量割合 $\\rho_i$ と RS 均衡値 $-Z$ によって算出した理想試行量割合 $\\rho^z_i$ との差分から確率分布を生成し、それに従って行動を選択する。負の割合を防ぐために調整パラメータ $b$ を導入する。 \\\\\n",
        "パラメータ $b$ の更新、SRS の定式、それによって導出される選択確率 $\\pi_i$ は以下のように。\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "b &>& \\frac{n_i}{\\rho^z_i} - N \\qquad \\cdots (15) \\\\\n",
        "SRS_i &=& (N+b)\\rho^z_i - n_i \\qquad \\cdots (16) \\\\\n",
        "\\pi_i &=& \\frac{SRS_i}{SRS_1 + SRS_2 + \\cdots + SRS_k} \\qquad \\cdots (17) \\\\\n",
        "\\end{eqnarray}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr8pWRq4wA7s"
      },
      "source": [
        "## 5. シミュレーションによる比較検証"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHLHF-55wIUK"
      },
      "source": [
        "### 5.1 実験概要\n",
        "\n",
        "シミュレーション数は 1,000、ステップ数は 100,000、腕の本数 $K$ は 20。評価指標は regret。報酬確率は一様分布から。 \\\\\n",
        "使用アルゴリズムは TS, RS($\\aleph = p_1$), RS-OPT, SRS($\\aleph = p_1$), SRS-OPT。 \\\\\n",
        "また非定常環境では、RS, SRS の代わりに忘却率 $\\gamma=0.999$ を用いた RS$\\gamma$, SRS$\\gamma$, RS$\\gamma$-OPT, SRS$\\gamma$-OPT を使用した。 \\\\\n",
        "非定常環境では 2,000step 毎に各選択肢の報酬確率がランダムに入れ替わる。どの選択肢も必ず別の報酬確率と入れ替わる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZAPsCknyhUB"
      },
      "source": [
        "### 5.2 実験結果\n",
        "\n",
        "定常環境では、SRS-OPT は RS-OPT, TS に劣る結果になった。10,000step までは TS よりもわずかに良い結果を示している。$\\aleph = p_1$ と設定した RS, SRS は互角の regret を示しており、100,000step においても性能は拮抗している。 \\\\\n",
        "非定常環境では、50,000step までは RS$\\gamma$-OPTが最も優位な結果を示したが、100,000step では SRS$\\gamma$-OPT が RS$\\gamma$-OPT よりも良い結果になった。$\\aleph = p_1$ と設定した RS$\\gamma$ と SRS$\\gamma$ では、全体を通して SRS$\\gamma$ の方が劣る結果になった。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73qbNC6IzDUh"
      },
      "source": [
        "## 6. 各アルゴリズムの選択比率についての検証"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1tGYgxHzIdM"
      },
      "source": [
        "### 6.1 実験概要\n",
        "\n",
        "定常環境で実験を行った。6,000step で選択肢数 $K$ は 4。1step で腕選択を 1,000回行わせて step毎の各腕の選択確率を観測した。各選択肢の確率 $p$ は固定値とした。 \\\\\n",
        "TS では ($p_1 = 0.6, p_2 = 0.5, p_3 = 0.4, p_4 = 0.3$) \\\\\n",
        "RS・SRS は ($p_1 = 0.55, p_2 = 0.50, p_3 = 0.45, p_4 = 0.40$) \\\\\n",
        "とした。また RS は $\\aleph_{opt}$ を用いた場合のみ観測し、SRS では $\\aleph_{opt}$と$\\aleph = p_1$ の場合について観測した。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_t_GtIr1LpS"
      },
      "source": [
        "### 6.2 実験結果\n",
        "\n",
        "TS の初めは大きく遷移しているがすぐに arm1 に収束している。 \\\\\n",
        "RS($\\aleph_{opt}$) では腕の選択確率が 1, 0 となり極端に遷移している。 \\\\\n",
        "SRS($\\aleph = p_1$) では長期に渡り大きな選択確率の遷移を繰り返しているが、3,000step 付近から arm1 に収束している。 \\\\\n",
        "SRS($\\aleph_{opt}$) でも初めは大きく遷移しているが SRS($\\aleph = p_1$) よりも早く arm1 に収束している。 \\\\\n",
        "\n",
        "いずれの SRS も、特に SRS-OPT はわずかであるが収束後も遷移を繰り返していることが分かる。(分からん) \\\\\n",
        "図6 だと 1, 0 になってないか？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29HvLHNw6HRu"
      },
      "source": [
        "## 7. 考察\n",
        "\n",
        "SRS の確率分布の生成に RS 均衡値を使用しているため $\\aleph \\geqq p_1$ の時にアルゴリズムは正しく機能するという予測だった。 \\\\\n",
        "定常環境では RS-OPT と SRS-OPT では差が生まれた一方、$\\aleph = p_1$ とした RS と SRSでは性能が拮抗していたため仮説通りだった。 \\\\\n",
        "非定常環境では反対に $\\aleph = p_1$ では差が生まれ、$\\aleph_{opt}$ では最初 RS$\\gamma$ が、最終的には SRS$\\gamma$ が優位になった。しかし、$\\aleph$ の値に関わらず、弧を描くようにグラフが伸びる RS に対し、SRS は直線状に伸びていたため step数を増やした場合、さらに差が開くと考えられる。(図 2 の SRS と SRS-OPT は直線、 RS と RS-OPT は弧) \\\\\n",
        "SRS は確率分布に従って行動選択を行うので決定論的に選択する RS とは違い、選択腕が定まった後も選択確率が遷移するので他の腕が選択される可能性が図 5 からみてとれる。(図 6 は分からん) \\\\\n",
        "非定常環境で step数が長期に渡った場合、SRS$\\gamma$-OPT が RS$\\gamma$-OPT を上回るのはこの特徴が有意に働き、RS よりも早く別の選択肢に切り替えられるからだと考えられる。 \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKm-R4Oi-rPK"
      },
      "source": [
        "## 8. 結論"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBn6NLHBgrDn"
      },
      "source": [
        "## 参考文献\n",
        "\n",
        "1. [learnlatex] https://colab.research.google.com/github/kalz2q/mycolabnotebooks/blob/master/learnlatex.ipynb#scrollTo=jZ4nUZbeqf2I"
      ]
    }
  ]
}